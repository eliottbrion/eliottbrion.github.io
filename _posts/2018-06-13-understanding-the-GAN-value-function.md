---
layout: default
title:  "Understanding the GAN value function"
author: "Chester"
---

I recently gained interest in Generative Adversarial Networks. Fascinated by both the theoritical idea and the various applications, I began the lecture of the orginal paper of Goodfellow et al. My enthusiasm came down as I got stuch at the first equation. This equation describes the minimax problem that a GAN aims to solve:

$$ \min_G \max_D V(D,G) = \mathbb E_{x \sim p_{data}(x)} [\log D(x)] + \mathbb E _{z \sim p_z (z)} [ \log(1-D(G(z))]$$ 

Despite my search on the web, I haven't found an explanation both convicing and simple. In this article, I propose to share an explanation that I finally found.

When training a GAN, we have samples ${ (x_i, y_i) }_ {i=1}^{2m} $ of images $x$ with their corresponding label ($y_i=1$ if $x_i$ has been drawn from $p_{data}$ and $y_i=0$ if $x_i$ has been drawn from $p_g$). Moreover, this sample contains the same number of true and generated data ($m$ of each). Let's suppose that the labels have been generated by a distribution $D^* (x) = P(Y=y | x)$. Our goal is to find this distribution $D^*$. Among all possible $D$, the one that generated the samples that we observe with the highest probability is:

$$D = \arg \max_D \prod_{i=1}^m P(y=y^{(i)} | x; D)$$

This is the maximum likelihood estimator for $D^* $. Since there are only two possible labels (for all $i$, $y^{(i)}$ is equal to either 1 or 0), this can be written as:

$$D = \arg \max_D \prod_{i:y^{(i)}=1} P(y=1 | x^{(i)}; D) \prod_{i:y^{(i)}=0} P(y=0 | x^{(i)})$$

By definition, any function $x \mapsto D(x)$ outputs the probability of $x$ being a "true" image rather than a generated one. Since an image is considered being either true or fake (there is no alternative), the probability $x$ being a "fake" image is $1-D(x)$. As a consequence,

$$D = \arg \max_D \prod_{i:y^{(i)}=1} D(x^{(i)}) \prod_{i:y^{(i)}=0} 1- D(x^{(i)})$$

Now since the function $x \mapsto \log x$ is a monotonically increasing function, solving this problem is equivalent to solving this is equivalent to:

$$D = \arg \max_D \log \left ( \prod_{i:y^{(i)}=1} D(x^{(i)}) \prod_{i:y^{(i)}=0} 1- D(x^{(i)}) \right )$$

Writing the logarithm of the product as the sum of the logarithm yields

$$D = \arg \max_D  \sum_{i:y^{(i)}=1} \log D(x^{(i)}) + \sum_{i:y^{(i)}=0} \log( 1- D(x^{(i)})) $$

This expression does not change is we multiply the quantity to be maximize by a number. Here we chose $1/m$:

$$D = \arg \max_D \frac{1}{m} \sum_{i:y^{(i)}=1} \log D(x^{(i)}) + \frac{1}{m} \sum_{i:y^{(i)}=0} \log( 1- D(x^{(i)})) $$

For all generated images (the $x^{(i)}$ whose associated label is $y{(i)}$, there exists a $z^{(i)}$ that has been drawn from $G$ such that $x_i = G(z^{(i)})$:

$$D = \arg \max_D \frac{1}{m} \sum_{i:y^{(i)}=1} \log D(x^{(i)}) + \frac{1}{m} \sum_{i:y^{(i)}=0} \log( 1- D(G(z^{(i)}))) $$

By the law of large numbers (LLN), $\frac{1}{m} \sum_{i=1}^m \log D(x^{(i)})$ gets arbitrarily close to $E_{x \sim p_{data}(x)} [\log D(x)]$ as $m$ increases to infinity (intuitively). We say that the left quantity converges in probability (weak version of the LLN) or almost surely (strong version of the LLN):

$$\frac{1}{m} \sum_{i=1}^m \log D(x^{(i)}) \rightarrow E_{x \sim p_{data}(x)} [\log D(x)]$$

Similarly,

$$ E_{x \sim p_{data}(x)} [\log( 1- D(G(z^{(i)})))]$$

This means that the distriminator aims to maximize is the log-likelihood with an infinity of samples:

$$ D = \arg \max_D = E_{x \sim p_{data}(x)} [\log( 1- D(G(z^{(i)})))]$$

The generator, on the contrary, aims to fool the distriminator. In other words, $D$ and ^G^play the following two-player minimax game with value function $V(G,D)$:

$$ \min_G \max_D V(D,G) = \mathbb E_{x \sim p_{data}(x)} [\log D(x)] + \mathbb E _{z \sim p_z (z)} [ \log(1-D(G(z))]$$ 

And we are done.

Hope that this helps you get started with the understanding of GANs ;)




