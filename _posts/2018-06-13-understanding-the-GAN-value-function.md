---
layout: default
title:  "Understanding the GAN value function"
author: "Chester"
---

I recently gained interest in Generative Adversarial Networks. Fascinated by both the theoritical idea and the various applications, I began reading the orginal paper of Goodfellow et al. My enthusiasm quickly calmed down as I got stuch at the first equation. This equation describes the minimax problem that a GAN aims to solve:

$$ \min_G \max_D V(D,G) = \mathbb E_{x \sim p_{data}(x)} [\log D(x)] + \mathbb E _{z \sim p_z (z)} [ \log(1-D(G(z))]$$ 

Despite my search on the web, I haven't found any explanation that is both convicing and simple. In this article, I propose to share an explanation that a few colleagues and I found.

When training a GAN, we have samples ${ (x_i, y_i) }_ {i=1}^{2m} $ of images $x$ with their corresponding label ($y_i=1$ if $x_i$ has been drawn from $p_{data}$ and $y_i=0$ if $x_i$ has been drawn from $p_g$). Moreover, there are the same number of true and generated images ($m$ of each). Let's suppose that the labels have been generated by a distribution $D^* (x) = P(Y=y | x)$. Our goal is to find this distribution $D^*$. Among all possible probability functions $D$, the one that generated the samples that we observe with the highest probability is

$$D = \arg \max_D \prod_{i=1}^m P(y=y^{(i)} | x; D)$$

This is the maximum likelihood estimator for $D^* $. Since there are only two possible labels (for all $i$, $y^{(i)}$ is equal to either 1 or 0), this can be written as

$$D = \arg \max_D \prod_{i:y^{(i)}=1} P(y=1 | x^{(i)}; D) \prod_{i:y^{(i)}=0} P(y=0 | x^{(i)})$$

By definition, any function $x \mapsto D(x)$ outputs the probability of $x$ being a "true" image rather than a generated one. Since an image is considered being either true or fake (there is no alternative), the probability $x$ being a "fake" image is $1-D(x)$. As a consequence,

$$D = \arg \max_D \prod_{i:y^{(i)}=1} D(x^{(i)}) \prod_{i:y^{(i)}=0} 1- D(x^{(i)})$$

Now since the function $x \mapsto \log x$ is a monotonically increasing function, solving this problem is equivalent to solving this is equivalent to

$$D = \arg \max_D \log \left ( \prod_{i:y^{(i)}=1} D(x^{(i)}) \prod_{i:y^{(i)}=0} 1- D(x^{(i)}) \right )$$

Writing the logarithm of the product as the sum of the logarithms yields

$$D = \arg \max_D  \sum_{i:y^{(i)}=1} \log D(x^{(i)}) + \sum_{i:y^{(i)}=0} \log( 1- D(x^{(i)})) $$

This expression does not change is we multiply the quantity to be maximized by any arbitrary number. Let's choose $1/m$

$$D = \arg \max_D \frac{1}{m} \sum_{i:y^{(i)}=1} \log D(x^{(i)}) + \frac{1}{m} \sum_{i:y^{(i)}=0} \log( 1- D(x^{(i)})) $$

For all generated images (for all $x^{(i)}$ such that $y^{(i)}=0$), there exists a $z^{(i)}$ that has been drawn from $p_z$ such that $x_i = G(z^{(i)})$

$$D = \arg \max_D \frac{1}{m} \sum_{i:y^{(i)}=1} \log D(x^{(i)}) + \frac{1}{m} \sum_{i:y^{(i)}=0} \log( 1- D(G(z^{(i)}))) $$

By the law of large numbers (LLN), $\frac{1}{m} \sum_{i=1}^m \log D(x^{(i)})$ gets arbitrarily close to $\mathbb{E}_ {x \sim p_{data}(x)} [\log D(x)]$ as $m$ increases to infinity (intuitively). We say that the left quantity *converges in probability* (weak version of the LLN) or *converges almost surely* (strong version of the LLN)

$$\frac{1}{m} \sum_{i=1}^m \log D(x^{(i)}) \xrightarrow{\text{$m \rightarrow \infty$}} \mathbb E_{x \sim p_{data}(x)} [\log D(x)]$$

Similarly,

$$ \frac{1}{m} \sum_{i=1}^m \log( 1- D(G(z^{(i)}))) \xrightarrow{\text{$m \rightarrow \infty$}} \mathbb{E}_ {x \sim p_{data}(x)} [\log( 1- D(G(z)))]$$

This means that the distriminator aims to maximize the log-likelihood with an infinte number of samples

$$ D = \arg \max_D \mathbb E_{x \sim p_{data}(x)} [\log D(x)] +  \mathbb{E} _{z \sim p_z (z)} [ \log(1-D(G(z))]$$

The generator, on the contrary, aims at fooling the distriminator. In other words, $D$ and ^G^play the following two-player minimax game with value function $V(G,D)$

$$ \min_G \max_D V(D,G) = \mathbb E_{x \sim p_{data}(x)} [\log D(x)] + \ \mathbb{E} _{z \sim p_z (z)} [ \log(1-D(G(z))] $$ 

And we are done.

Hope that this helps you get started with the understanding of GANs ;)




