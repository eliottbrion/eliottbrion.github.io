---
layout: default
title:  "Understanding the GAN value function"
author: "Chester"
---

I recently gained interest for Generative Adversarial Networks. Fascinated by both the theoritical idea and the various applications, I began the lecture of the orginal paper of Goodfellow et al. My enthousiams came down as I got stuch at the first equation. This equation describes the minimax problem that a GAN solves:

$$ \min_G \max_D V(D,G) = \mathbb E_{x \sim p_{data}(x)} [\log D(x)] + \mathbb E _{z \sim p_z (z)} [ \log(1-D(G(z))]$$ 

Despite my search on the web, I haven't found an explanation both convicing and simple. In this article, I propose to share an explanation that I finally found.

Let suppose you have samples $\{ (x_i, y_i) \}_ {i=1}^n $ of images $x$ with their corresponding label ($y_i=1$ if $x_i$ has been drawn from $p_data$ and $y_i=0$ if $x_i$ has been drawn from $p_g$). Let's suppose that the labels have been generated by a distribution $D*(x) = P(Y=y | x)$. Our goal is to find this distribution $D^*$. Among all possible $D$, the one that generated the samples that we observe with the highest probability is:

$$D = \arg \max_D \prod_{i=1}^m P(y=y^{(i)} | x; D)$$

Since there are only two possible labels (for all $i$, $y^{(i)}$ is equal to either 1 or 0), this can be written as:

$$D = \arg \max_D \prod_{i:y^{(i)}=1} P(y=1 | x^{(i)}; D) \prod_{i:y^{(i)}=0} P(y=0 | x^{(i)})$$

By definition, any function $x \mapsto D(x)$ outputs the probability of $x$ being a "true" image rather than a generated one, i.e. $D(x)$ . Since an image is considered being either true or fake (there is no alternative), the probability $x$ being a "fake" image is $1-D(x)$. As a consequence,

$$D = \arg \max_D \prod_{i:y^{(i)}=1} D(x^{(i)}) \prod_{i:y^{(i)}=0} 1- D(x^{(i)})$$

Now since the function $x \mapsto \log x$ is a monotonically increasing function, solving this problem is equivalent to solving this is equivalent to:

$$D = \arg \max_D \log \left ( \prod_{i:y^{(i)}=1} D(x^{(i)}) \prod_{i:y^{(i)}=0} 1- D(x^{(i)}) \right )$$

Writing the logarithm of the product as the sum of the logarithm yields

$$D = \arg \max_D  \sum{i:y^{(i)}=1} \log D(x^{(i)}) \prod_{i:y^{(i)}=0} \log( 1- D(x^{(i)})) $$
